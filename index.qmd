---
title: "Lecture Notes Draft"
subtitle: "Diffusion Models for weather extremes prediction"
author: Joe Bobby, Andrew Bierbower
date: 2025-11-24
type: "lecture notes"
module: 1
week: 1
topics: ["Diffusion models"]
objectives:
  - "Show the working principle of diffusion models"
  - "Explore applications of Diffusion models"
ps_connection: "We attempt to show the utility of using Diffusion models for applications in weather inpainting and in temperature-precipitation modeling"

engine: julia
julia:
  exeflags: ["+1.11"] # ensures version 1.11

format:
  html:
    toc: true
    toc-depth: 2
    code-block-bg: "#f8f8f8"
    code-block-border-left: "#e1e4e5"
    theme: simplex
    number-sections: true
    fig-format: svg
    code-annotations: hover
    code-line-numbers: true
    date-format: "ddd., MMM. D"
  typst:
    fontsize: 11pt
    margin: 
      x: 1in
      y: 1in
    number-sections: true
    fig-format: svg
    code-line-numbers: true
    footer: "{{< meta author >}}"
    date-format: "ddd., MMM. D"

execute: 
  cache: true

# Code formatting options
code-overflow: wrap
code-line-numbers: true
code-block-font-size: "0.85em"

bibliography: references.bib  # Path to your BibTeX file
---

## The Basics of Diffusion Modeling

Diffusion models are a class of generative deep learning models (like GANs or VAEs), but function differently. They are probabilistic and model the entire distribution of possible outcomes, allowing for the generation of diverse, physically plausible realizations.

They way they work is through the combination of a forward and backward process that destroys and created information respectively.

Forward Process (Destroying information): Imagine taking a clear image (or climate map) and slowly adding Gaussian noise over many steps until it becomes pure static (random noise).

Reverse Process (Creating Information): The neural network learns to reverse this process. It starts with pure noise and iteratively denoises it to reconstruct a plausible sample from the data distribution.

## Forward Diffusion Formula in Quarto

The noisy data at each of the forward steps can be modelled using a stochastic noising process. The equation describing the noisy data at time step $t$, $\mathbf{x}_t$, in terms of the original data, $\mathbf{x}_0$, and added noise, $\boldsymbol{\epsilon}$, is given by:

$$
\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \cdot \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \cdot \boldsymbol{\epsilon}
$$

### Key Variables

* **$\mathbf{x}_t$**: The **noisy data** at time step $t$ (what the model sees during training).
* **$\mathbf{x}_0$**: The **original, clean data** (the *signal*, e.g., the original daily temperature map).
* **$\boldsymbol{\epsilon}$**: The **random noise** added (pure static), typically sampled from a standard normal distribution.
* **$\bar{\alpha}_t$** (read as **alpha-bar sub $t$**): A **schedule value** between 0 and 1. It controls the **signal-to-noise ratio**. As $t$ increases (more steps of diffusion), $\bar{\alpha}_t$ **decreases**, resulting in less signal and more noise in $\mathbf{x}_t$.

## The Reverse Process (Denoising)

The **reverse process** is the **learning objective** of the diffusion model. The neural network, often a U-Net architecture, is trained to iteratively *undo* the noise added during the forward process. This approach makes sense because after the model has trained, given a completely noisy image, the reverse process has the ability to **sample the distribution of the domain** learnt by the model.

It trains by trying to **predict the noise ($\boldsymbol{\epsilon}$)** that was added to $\mathbf{x}_0$ to create $\mathbf{x}_t$. If it can predict the noise perfectly, it can subtract it from $\mathbf{x}_t$ to reveal the clean, original image ($\mathbf{x}_0$).

---

### Loss Function (Mean Squared Error)

The model's training objective is minimized using the following simple $\mathbf{L_2}$ loss function (Mean Squared Error), which measures the difference between the true noise and the predicted noise.

$$
\text{Loss} = || \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_t, t) ||^2
$$

### Key Variables in the Loss

* **$\boldsymbol{\epsilon}$**: The **actual noise** that was added (the **ground truth**).
* **$\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_t, t)$**: The neural network's **prediction** of the noise, where $\theta$ represents the network's trainable parameters, and the prediction depends on the noisy data $\mathbf{x}_t$ and the time step $t$.
* **$|| \ldots ||^2$**: This is the **squared $\mathbf{L_2}$ norm** (Mean Squared Error). It calculates the square of the difference between the actual noise and the predicted noise across all dimensions (pixels/features). The model aims to drive this loss value **as close to zero as possible**.

## Comparison with Generative Adversarial Networks (GANs)

To fully appreciate the utility of diffusion models in scientific emulation, it is necessary to contrast them with the previous state-of-the-art: Generative Adversarial Networks (GANs). While GANs are computationally faster at inference time, they suffer from structural issues that make them less suitable for capturing the complex statistics of climate data.

### 1. Structural Differences
* **GANs (The "Forgery" Game):** GANs rely on a minimax game between two networks: a *Generator* (creating fakes) and a *Discriminator* (detecting fakes). The training signal comes solely from the Discriminator's ability to spot errors.
* **Diffusion (The "Sculptor"):** Diffusion models use a single network with a stable, supervised learning objective (Mean Squared Error of noise). There is no adversary; the model simply learns to denoise incrementally.

### 2. The Problem of "Mode Collapse"
A critical failure mode in GANs is **mode collapse**, where the Generator finds a single type of output that successfully fools the Discriminator and produces it repeatedly (e.g., generating only one specific type of storm).
* **Relevance to Climate:** In climate science, we care about the "tails" of the distribution—rare events like 1-in-100-year floods. A GAN might ignore these rare events to focus on easier, average weather.
* **Diffusion Advantage:** Because diffusion models maximize the likelihood of the training data (mathematically similar to maximizing the Evidence Lower Bound or ELBO), they are forced to learn the **entire distribution**, ensuring that rare extreme events are possible in the generated output.

## Case Study: DiffESM (Bassetti et al., 2024)

This module focuses on the practical application of diffusion in the paper *DiffESM: Conditional Emulation of Temperature and Precipitation in Earth System Models*. This work addresses the trade-off between the high accuracy of Earth System Models (ESMs) and their prohibitive computational cost.

### The Challenge: Emulating Complexity
Running a full ESM involves solving complex partial differential equations (fluid dynamics, thermodynamics) on supercomputers. A single simulation can take months. To perform risk assessments, scientists need thousands of possible future weather scenarios (ensembles), which is computationally impossible with full ESMs.

Existing statistical emulators often simplify the problem by predicting **monthly averages**. However, a monthly average of $25^\circ\text{C}$ could mean 30 days of $25^\circ\text{C}$ (mild) or 15 days of $40^\circ\text{C}$ and 15 days of $10^\circ\text{C}$ (catastrophic). DiffESM aims to recover this lost daily variability.

### Model Architecture: The 3D U-Net
DiffESM adapts the standard image-generation architecture for spatiotemporal data.

* **Dimensionality ($T \times H \times W$):** Instead of 2D images, the model processes 3D tensors representing **Time (Days) $\times$ Latitude $\times$ Longitude**.
* **The U-Net Backbone:** The network uses a "U" shape with downsampling layers (contracting path) to capture broad context (e.g., "a large low-pressure system") and upsampling layers (expanding path) to localize details (e.g., "intense rain at this specific coordinate").
* **Conditioning ($c$):** The crucial innovation is **Conditioning**. The model receives the **Monthly Mean Map** as an extra input channel. This acts as a hard constraint: the model is free to "hallucinate" daily weather patterns, but they must mathematically average out to match the provided monthly mean.

## Advanced Implementation Details

Applying diffusion to scientific data requires specific mathematical adjustments beyond the standard "image generation" tutorials. DiffESM implements three key technical modifications:

### 1. Velocity Prediction ($v$-parameterization)
Standard diffusion models predict the noise $\boldsymbol{\epsilon}$. However, for climate data, the transition from "pure signal" to "pure noise" can be numerically unstable. DiffESM uses **$v$-prediction** (Salimans & Ho, 2022), where the model predicts a velocity vector $\mathbf{v}$ in the latent space:

$$
\mathbf{v}_t \equiv \sqrt{\bar{\alpha}_t}\boldsymbol{\epsilon} - \sqrt{1 - \bar{\alpha}_t}\mathbf{x}_0
$$

This formulation provides a more consistent target for the neural network across all time steps $t$, preventing loss spikes during training and improving the convergence on high-variance precipitation data.

### 2. Accelerated Sampling (DPMSolver++)
A major bottleneck of diffusion is the inference speed. The standard reverse process requires solving a stochastic differential equation (SDE) over 1,000 small steps.
* **The Optimization:** DiffESM utilizes **DPMSolver++**, a high-order solver for the probability flow ODE.
* **The Result:** This allows the model to traverse the reverse process in just **20 to 25 steps** instead of 1,000, reducing the generation time for a global climate simulation from minutes to seconds without sacrificing accuracy.

### 3. Data Transformation (The "Drizzle" Fix)
Precipitation data is highly non-Gaussian; it contains many zeros (dry days) and a few extreme values (storms). Neural networks trained on raw precipitation often default to predicting "drizzle" (tiny amounts of rain everywhere) to minimize error.
* **Cubic Root Transformation:** The authors apply a transformation $x \to x^{1/3}$ to compress the heavy tail of the distribution, making extreme storms easier for the model to learn.
* **Thresholding:** During generation, a post-processing step sets any value $< 1 \text{mm/day}$ to zero, artificially restoring the natural "dry spells" that strictly continuous models often smooth out.

## Evaluation: Physics-Informed Metrics

Evaluating a generative model in science is different from evaluating art. We do not care if the image "looks good"; we care if it obeys physical statistics. DiffESM is evaluated using an **"Oracle" strategy**, comparing its errors to the internal variability of the ESM itself (Held-Out realizations).

### Key Physical Metrics
1.  **Kolmogorov-Smirnov (KS) Test:** A non-parametric test that compares the cumulative distribution function (CDF) of the generated weather against the actual ESM data. A low KS statistic indicates that the model correctly captures the probability of both common and rare events.
2.  **SDII (Simple Precipitation Intensity Index):** This measures the average precipitation intensity on wet days. It ensures the model generates **intense, realistic storms** rather than just spreading the water out over time (the "drizzle" problem).
3.  **Temporal Persistence (Streaks):** The model is tested on its ability to generate consecutive extremes:
    * **Hot Streaks:** Consecutive days above the 90th percentile temperature.
    * **Dry Spells:** Consecutive days with $<1$mm of rain.
    * Success here proves the 3D U-Net has learned the *temporal inertia* of weather systems—that a heat wave tends to last for days.

## Applications in Weather Extremes

Based on recent discussions regarding the utility of diffusion models in climate science, we focus on two primary modeling approaches: probabilistic global forecasting and precipitation data inpainting.



### Precipitation Data Inpainting

The second application[@kishikawa2025conditionaldiffusionmodelsglobal] addresses the challenge of reconstructing global precipitation maps, which often suffer from extensive spatio-temporal gaps due to the orbital limitations of polar-orbiting satellites used by products like GSMaP and IMERG.
  ![Alternative text for accessibility](images\Screenshot 2025-12-05 234114.png)
  
  Above is an example of forward and reverse diffusion processes applied to a global precipitation map. Forward diffusion corrupts the precipitation map sequence x0 by successive noise additions (upper), and reverse diffusion reconstructs the predicted sequence x0 through iterative denoising, restoring observed regions to their original values

* **Conditional Diffusion Framework:**
    This approach formulates the completion of precipitation maps as a **video inpainting task**[@videoinpaintingweatherreconstruction2024]. It utilizes a **Conditional Denoising Diffusion Probabilistic Model (DDPM)** backbone to generate physically consistent data for unobserved regions.
    
* **3D U-Net Architecture:**
    Unlike 2D image models, this method employs a **3D U-Net architecture** with 3D convolutions. This allows the model to simultaneously capture spatial patterns and temporal dynamics, ensuring that the inpainted rainfall moves and evolves realistically over time.

* **Multi-Modal Conditioning:**
    To achieve high accuracy, the model does not guess blindly. It uses a **3D condition encoder** to incorporate multiple data sources as guidance:
    * **Masked Precipitation Sequence:** The primary input containing known data.
    * **Infrared (IR) Cloud Imagery:** Provides critical cues for cloud cover in regions where microwave sensors have gaps.
    * **Topography (ETOPO):** Helps the model account for orographic effects (e.g., mountains forcing air up to create rain).
    * **Spatio-Temporal Metadata:** Latitude, longitude, and physical time inputs ensure the weather patterns match the specific location and season.

* **Advantages over Conventional Methods:**
    Traditional methods like Inverse Distance Weighting (IDW)[@IDWerrorpropagationDEMfromanaloguetopographicalmap] or Kriging[@SHTILIYANOVA2017440] often produce overly smooth results that lack realistic texture and fail to capture complex storm structures. This diffusion-based approach outperforms them by generating sharp, high-fidelity precipitation fields that maintain both spatial and temporal continuity.

* **Results:**
    Some results from the paper [@kishikawa2025conditionaldiffusionmodelsglobal]:
    ![Alternative text for accessibility](images\Screenshot 2025-12-05 234235.png)
    
    Comparison of the input masked sequence(left), the inpainted sequence(middle), and the groundtruth sequence(right), using example data from the ERA5 precipitation maps
