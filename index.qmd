---
title: "Lecture Notes Draft"
subtitle: "Diffusion Models for weather extremes prediction"
author: Joe Bobby, Andrew Bierbower
date: 2025-11-24
type: "lecture notes"
module: 1
week: 1
topics: ["Diffusion models"]
objectives:
  - "Show the working principle of diffusion models"
  - "Explore applications of Diffusion models"
ps_connection: "We attempt to show the utility of using Diffusion models for applications in weather inpainting and in temperature-precipitation modeling"

engine: julia
julia:
  exeflags: ["+1.11"] # ensures version 1.11

format:
  html:
    toc: true
    toc-depth: 2
    code-block-bg: "#f8f8f8"
    code-block-border-left: "#e1e4e5"
    theme: simplex
    number-sections: true
    fig-format: svg
    code-annotations: hover
    code-line-numbers: true
    date-format: "ddd., MMM. D"
  typst:
    fontsize: 11pt
    margin: 
      x: 1in
      y: 1in
    number-sections: true
    fig-format: svg
    code-line-numbers: true
    footer: "{{< meta author >}}"
    date-format: "ddd., MMM. D"

execute: 
  cache: true

# Code formatting options
code-overflow: wrap
code-line-numbers: true
code-block-font-size: "0.85em"

bibliography: references.bib  # Path to your BibTeX file
---

## The Basics of Diffusion Modeling

Diffusion models are a class of generative deep learning models (like GANs or VAEs), but function differently. They are probabilistic and model the entire distribution of possible outcomes, allowing for the generation of diverse, physically plausible realizations.

They way they work is through the combination of a forward and backward process that destroys and created information respectively.

Forward Process (Destroying information): Imagine taking a clear image (or climate map) and slowly adding Gaussian noise over many steps until it becomes pure static (random noise).

Reverse Process (Creating Information): The neural network learns to reverse this process. It starts with pure noise and iteratively denoises it to reconstruct a plausible sample from the data distribution.

## Forward Diffusion Formula in Quarto

The noisy data at each of the forward steps can be modelled using a stochastic noising process. The equation describing the noisy data at time step $t$, $\mathbf{x}_t$, in terms of the original data, $\mathbf{x}_0$, and added noise, $\boldsymbol{\epsilon}$, is given by:

$$
\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \cdot \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \cdot \boldsymbol{\epsilon}
$$

### Key Variables

* **$\mathbf{x}_t$**: The **noisy data** at time step $t$ (what the model sees during training).
* **$\mathbf{x}_0$**: The **original, clean data** (the *signal*, e.g., the original daily temperature map).
* **$\boldsymbol{\epsilon}$**: The **random noise** added (pure static), typically sampled from a standard normal distribution.
* **$\bar{\alpha}_t$** (read as **alpha-bar sub $t$**): A **schedule value** between 0 and 1. It controls the **signal-to-noise ratio**. As $t$ increases (more steps of diffusion), $\bar{\alpha}_t$ **decreases**, resulting in less signal and more noise in $\mathbf{x}_t$.

## The Reverse Process (Denoising)

The **reverse process** is the **learning objective** of the diffusion model. The neural network, often a U-Net architecture, is trained to iteratively *undo* the noise added during the forward process. This approach makes sense because after the model has trained, given a completely noisy image, the reverse process has the ability to **sample the distribution of the domain** learnt by the model.

It trains by trying to **predict the noise ($\boldsymbol{\epsilon}$)** that was added to $\mathbf{x}_0$ to create $\mathbf{x}_t$. If it can predict the noise perfectly, it can subtract it from $\mathbf{x}_t$ to reveal the clean, original image ($\mathbf{x}_0$).

---

### Loss Function (Mean Squared Error)

The model's training objective is minimized using the following simple $\mathbf{L_2}$ loss function (Mean Squared Error), which measures the difference between the true noise and the predicted noise.

$$
\text{Loss} = || \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_t, t) ||^2
$$

### Key Variables in the Loss

* **$\boldsymbol{\epsilon}$**: The **actual noise** that was added (the **ground truth**).
* **$\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_t, t)$**: The neural network's **prediction** of the noise, where $\theta$ represents the network's trainable parameters, and the prediction depends on the noisy data $\mathbf{x}_t$ and the time step $t$.
* **$|| \ldots ||^2$**: This is the **squared $\mathbf{L_2}$ norm** (Mean Squared Error). It calculates the square of the difference between the actual noise and the predicted noise across all dimensions (pixels/features). The model aims to drive this loss value **as close to zero as possible**.

## Applications in Weather Extremes

Based on recent discussions regarding the utility of diffusion models in climate science, we focus on two primary modeling approaches: probabilistic global forecasting and precipitation data inpainting.



### Precipitation Data Inpainting

The second application[@kishikawa2025conditionaldiffusionmodelsglobal] addresses the challenge of reconstructing global precipitation maps, which often suffer from extensive spatio-temporal gaps due to the orbital limitations of polar-orbiting satellites used by products like GSMaP and IMERG.
  ![Alternative text for accessibility](images\Screenshot 2025-12-05 234114.png)
  
  Above is an example of forward and reverse diffusion processes applied to a global precipitation map. Forward diffusion corrupts the precipitation map sequence x0 by successive noise additions (upper), and reverse diffusion reconstructs the predicted sequence x0 through iterative denoising, restoring observed regions to their original values

* **Conditional Diffusion Framework:**
    This approach formulates the completion of precipitation maps as a **video inpainting task**[@videoinpaintingweatherreconstruction2024]. It utilizes a **Conditional Denoising Diffusion Probabilistic Model (DDPM)** backbone to generate physically consistent data for unobserved regions.
    
* **3D U-Net Architecture:**
    Unlike 2D image models, this method employs a **3D U-Net architecture** with 3D convolutions. This allows the model to simultaneously capture spatial patterns and temporal dynamics, ensuring that the inpainted rainfall moves and evolves realistically over time.

* **Multi-Modal Conditioning:**
    To achieve high accuracy, the model does not guess blindly. It uses a **3D condition encoder** to incorporate multiple data sources as guidance:
    * **Masked Precipitation Sequence:** The primary input containing known data.
    * **Infrared (IR) Cloud Imagery:** Provides critical cues for cloud cover in regions where microwave sensors have gaps.
    * **Topography (ETOPO):** Helps the model account for orographic effects (e.g., mountains forcing air up to create rain).
    * **Spatio-Temporal Metadata:** Latitude, longitude, and physical time inputs ensure the weather patterns match the specific location and season.

* **Advantages over Conventional Methods:**
    Traditional methods like Inverse Distance Weighting (IDW)[@IDWerrorpropagationDEMfromanaloguetopographicalmap] or Kriging[@SHTILIYANOVA2017440] often produce overly smooth results that lack realistic texture and fail to capture complex storm structures. This diffusion-based approach outperforms them by generating sharp, high-fidelity precipitation fields that maintain both spatial and temporal continuity.

* **Results:**
    Some results from the paper [@kishikawa2025conditionaldiffusionmodelsglobal]:
    ![Alternative text for accessibility](images\Screenshot 2025-12-05 234235.png)
    
    Comparison of the input masked sequence(left), the inpainted sequence(middle), and the groundtruth sequence(right), using example data from the ERA5 precipitation maps
